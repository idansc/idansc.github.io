
<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Idan Schwartz">

  
  
  
    
  
  <meta name="description" content="PhD, AI Researcher">

  
  <link rel="alternate" hreflang="en-us" href="https://idansc.github.io/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.bb6ecd6e041101096a3d9bda51721642.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'G-0G7KF9T2ZC', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Idan Schwartz">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Idan Schwartz">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://idansc.github.io/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@idansc">
  <meta property="twitter:creator" content="@idansc">
  
  <meta property="og:site_name" content="Idan Schwartz">
  <meta property="og:url" content="https://idansc.github.io/">
  <meta property="og:title" content="Idan Schwartz">
  <meta property="og:description" content="PhD, AI Researcher"><meta property="og:image" content="https://idansc.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2021-09-01T00:00:00&#43;00:00">
  

  

  

  <title>Idan Schwartz</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Idan Schwartz</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications" data-target="#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact" data-target="#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>



<span class="js-widget-page d-none"></span>




  







  
  
  

  

  

  

  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="about" class="home-section wg-about   "  >
    <div class="container">
      




  









<div class="row" itemprop="author" itemscope itemtype="http://schema.org/Person" itemref="person-email person-telephone person-address">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="portrait" src="/authors/admin/avatar_hu88308e6ce9c4ba602923a151633d51bc_729424_250x250_fill_lanczos_center_3.png" itemprop="image" alt="Avatar">
      

      <div class="portrait-title">
        <h2 itemprop="name">Idan Schwartz</h2>
        <h3 itemprop="jobTitle">PhD, AI Researcher</h3>

        
        <h3 itemprop="worksFor" itemscope itemtype="http://schema.org/Organization">
          <a href="http://www.cs.technion.ac.il/" target="_blank" itemprop="url" rel="noopener">
          <span itemprop="name">Technion, CS department</span>
          </a>
        </h3>
        
      </div>

      <link itemprop="url" href="">

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
        
        
        
        
          
        
        <li>
          <a itemprop="sameAs" href="https://scholar.google.com/citations?user=5V-yJT4AAAAJ&amp;hl=en" target="_blank" rel="noopener">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a itemprop="sameAs" href="https://github.com/idansc" target="_blank" rel="noopener">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a itemprop="sameAs" href="https://www.quora.com/profile/Idan-Schwartz" target="_blank" rel="noopener">
            <i class="fab fa-quora big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a itemprop="sameAs" href="https://twitter.com/idansc" target="_blank" rel="noopener">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8" itemprop="description">

    
    <h1>Research &amp; Bio</h1>

    <p>Idan&rsquo;s research is centered around cognition in deep learning models. This includes attention models, multimodal problems, and multimodal bias removal. Specifically in AI tasks; such as, <!-- raw HTML omitted -->visual question answering<!-- raw HTML omitted --> and <!-- raw HTML omitted -->visual dialog<!-- raw HTML omitted -->.</p>
<p>Idan Schwartz is a doctoral candidate in the Computer Science department at the Technion, working with <!-- raw HTML omitted -->Alexander G. Schwing<!-- raw HTML omitted --> and <!-- raw HTML omitted -->Tamir Hazan<!-- raw HTML omitted -->. In 2016 he joined eBay as a researcher, applying computer vision and natural language processing solutions for eBay&rsquo;s catalog. In 2019 he joined Microsoft as a researcher at Search, Assistant, and Intelligence group. In 2020, he joined Spot (acquired by NetApp) to lead the research group.</p>


    <div class="row">

      
      <div class="col-md-5">
        <h3>Interests</h3>
        <ul class="ul-interests">
          
          <li>Artificial Intelligence &amp; Cognition</li>
          
          <li>Attention Models</li>
          
          <li>Multimodal Learning</li>
          
          <li>Computer Vision</li>
          
          <li>Natural Language Processing</li>
          
        </ul>
      </div>
      

      
      <div class="col-md-7">
        <h3>Education</h3>
        <ul class="ul-edu fa-ul">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD in Computer Science, 2020</p>
              <p class="institution">Technion</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BSc in Computer Science, 2015</p>
              <p class="institution">Technion</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>

    </div>
  </section>

  
  
  

  

  

  

  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="publications" class="home-section wg-pages   "  >
    <div class="container">
      








  























  





  


<div class="row">
  <div class="col-12 col-lg-4 section-heading">
    <h1>Publications</h1>
    
  </div>
  <div class="col-12 col-lg-8">

    <p><!-- raw HTML omitted -->
</p>

    
      
        






  
  





  


<div class="card-simple" itemscope itemtype="http://schema.org/ScholarlyArticle">

  
    



<meta content="2021-09-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2021-10-02 17:41:47 &#43;0300 IDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-gat/">I. Gat</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-schwartz/">I. Schwartz</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/a.-~g.-schwing/">A. ~G. Schwing</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>September 2021</time>
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>NeurIPS'21</em>
    
  </span>
  

  

  
  

  
  

  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3" itemprop="name">
    <a href="/publication/gat-neurips-2021/" itemprop="url">Perceptual Score: Measuring Perceptiveness of Multi-Modal Classifiers</a>
  </h3>

  
  <div class="article-style" itemprop="articleBody">
    Machine learning advances in the last decade have relied significantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classifiers, in turn, are prone to exploit those biases and to find shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we find a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classifiers to start quantifying perceptiveness via the proposed perceptual score.
  </div>
  

  
  <div class="btn-links">
    








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/gat-neurips-2021/gat-neurips-2021.bib">
  Cite
</button>















  </div>
  

</div>

      
    
      
        






  
  





  


<div class="card-simple" itemscope itemtype="http://schema.org/ScholarlyArticle">

  
    



<meta content="2021-06-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2021-10-02 17:28:05 &#43;0300 IDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-schwartz/">I. Schwartz</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>June 2021</time>
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>NAACL'21</em>
    
  </span>
  

  

  
  

  
  

  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3" itemprop="name">
    <a href="/publication/schwartz-naacl-2021/" itemprop="url">Ensemble of MRR and NDCG models for Visual Dialog</a>
  </h3>

  
  <div class="article-style" itemprop="articleBody">
    Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., &lsquo;yeah&rsquo; and &lsquo;yes&rsquo;). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as &lsquo;I don&rsquo;t know&rsquo;. Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge.
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2104.07511.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/schwartz-naacl-2021/schwartz-naacl-2021.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/idansc/mrr-ndcg" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

      
    
      
        






  
  





  


<div class="card-simple" itemscope itemtype="http://schema.org/ScholarlyArticle">

  
    



<meta content="2020-10-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2021-10-02 17:28:05 &#43;0300 IDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-gat/">I. Gat</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-schwartz/">I. Schwartz</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/a.-~g.-schwing/">A. ~G. Schwing</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/t.-hazan/">T. Hazan</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>October 2020</time>
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>NeurIPS'20</em>
    
  </span>
  

  

  
  

  
  

  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3" itemprop="name">
    <a href="/publication/gat-neurips-2020/" itemprop="url">Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies</a>
  </h3>

  
  <div class="article-style" itemprop="articleBody">
    Many recent datasets contain a variety of different data modalities, for instance, image, question, and answer data in visual question answering (VQA). When training deep net classifiers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classification results than others. This is suboptimal because the classifier is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classification result. However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2, and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efficacy of our method on Colored MNIST.
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2010.10802.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/gat-neurips-2020/gat-neurips-2020.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/itaigat/removing-bias-in-multi-modal-classifiers" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

      
    
      
        






  
  





  


<div class="card-simple" itemscope itemtype="http://schema.org/ScholarlyArticle">

  
    



<meta content="2019-06-12 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2021-10-02 17:28:05 &#43;0300 IDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-schwartz/">I. Schwartz</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/a.-~g.-schwing/">A. ~G. Schwing</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/t.-hazan/">T. Hazan</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>June 2019</time>
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>CVPR'19</em>
    
  </span>
  

  

  
  

  
  

  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3" itemprop="name">
    <a href="/publication/schwartz-cvpr-2019/" itemprop="url">A Simple Baseline for Audio-Visual Scene-Aware Dialog</a>
  </h3>

  
  <div class="article-style" itemprop="articleBody">
    The recently proposed audio-visual scene-aware dialog task paves the way to a more data-driven way of learning virtual assistants, smart speakers and car navigation systems. However, very little is known to date about how to effectively extract meaningful information from a plethora of sensors that pound the computational engine of those devices. Therefore, in this paper, we provide and carefully analyze a simple baseline for audio-visual scene-aware dialog which is trained end-to-end. Our method differentiates in a data-driven manner useful signals from distracting ones using an attention mechanism. We evaluate the proposed approach on the recently introduced and challenging audio-visual scene-aware dataset, and demonstrate the key features that permit to outperform the current state-of-the-art by more than 20% on CIDEr.
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/1904.05876.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/schwartz-cvpr-2019/schwartz-cvpr-2019.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/idansc/simple-avsd" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

      
    
      
        






  
  





  


<div class="card-simple" itemscope itemtype="http://schema.org/ScholarlyArticle">

  
    



<meta content="2019-06-12 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2021-10-02 17:28:05 &#43;0300 IDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-schwartz/">I. Schwartz</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/a.-~g.-schwing/">A. ~G. Schwing</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/t.-hazan/">T. Hazan</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>June 2019</time>
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>CVPR'19</em>
    
  </span>
  

  

  
  

  
  

  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3" itemprop="name">
    <a href="/publication/schwartz-fgacvpr-2019/" itemprop="url">Factor Graph Attention</a>
  </h3>

  
  <div class="article-style" itemprop="articleBody">
    Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/1904.05880.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/schwartz-fgacvpr-2019/schwartz-fgacvpr-2019.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/idansc/fga" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

      
    
      
        






  
  





  


<div class="card-simple" itemscope itemtype="http://schema.org/ScholarlyArticle">

  
    



<meta content="2017-12-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2021-10-02 17:28:05 &#43;0300 IDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/i.-schwartz/">I. Schwartz</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/a.-~g.-schwing/">A. ~G. Schwing</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/t.-hazan/">T. Hazan</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>December 2017</time>
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>NIPS'17</em>
    
  </span>
  

  

  
  

  
  

  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3" itemprop="name">
    <a href="/publication/schwartz-nips-2017/" itemprop="url">High-Order Attention Models for Visual Question Answering</a>
  </h3>

  
  <div class="article-style" itemprop="articleBody">
    The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.
  </div>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/1711.04323" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/schwartz-nips-2017/schwartz-nips-2017.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/idansc/HighOrderAtten" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

      
    

    

  </div>
</div>

    </div>
  </section>

  
  
  

  

  

  

  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="contact" class="home-section wg-contact   "  >
    <div class="container">
      




<div class="row contact-widget">
  <div class="col-12 col-lg-4 section-heading">
    <h1>Contact</h1>
    
  </div>
  <div class="col-12 col-lg-8">
    

    

    <ul class="fa-ul" itemscope>

      
      <li>
        <i class="fa-li fas fa-envelope fa-2x" aria-hidden="true"></i>
        <span id="person-email" itemprop="email">idanschwartz at gmail dot com</span>
      </li>
      

      
      <li>
        <i class="fa-li fas fa-phone fa-2x" aria-hidden="true"></i>
        <span id="person-telephone" itemprop="telephone">(972)-4-829-4960</span>
      </li>
      

      
      <li>
        <i class="fa-li fas fa-map-marker fa-2x" aria-hidden="true"></i>
        <span id="person-address" itemprop="address">Taub Room 532, Technion, Israel</span>
      </li>
      

      

      

      
      
      
      
      
        
      
      
      
      
      
      <li>
        <i class="fa-li fab fa-skype fa-2x" aria-hidden="true"></i>
        <a href="skype:idanschwartz?chat" >Skype Me</a>
      </li>
      

    </ul>

    
    <div class="d-none">
      <input id="map-provider" value="2">
      <input id="map-lat" value="32.7777324">
      <input id="map-lng" value="35.0216216">
      <input id="map-dir" value="Taub Room 532, Technion, Israel">
      <input id="map-zoom" value="15">
      <input id="map-api-key" value="">
    </div>
    <div id="map"></div>
    

  </div>
</div>

    </div>
  </section>


<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.430d01b7a0714748d7efee07176e9c11.js"></script>

  </body>
</html>


