[{"authors":["admin"],"categories":null,"content":"Idan\u0026rsquo;s research is centered around cognition in deep learning models. This includes attention models, multimodal problems, and multimodal bias removal. Specifically in AI tasks; such as, visual question answeringand visual dialog.\nIdan Schwartz is a doctoral candidate in the Computer Science department at the Technion, working with Alexander G. Schwingand Tamir Hazan. In 2016 he joined eBay as a researcher, applying computer vision and natural language processing solutions for eBay\u0026rsquo;s catalog. In 2019 he joined Microsoft as a researcher at Search, Assistant, and Intelligence group. In 2020, he joined Spot (acquired by NetApp) to lead the research group.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633184885,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://idansc.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Idan\u0026rsquo;s research is centered around cognition in deep learning models. This includes attention models, multimodal problems, and multimodal bias removal. Specifically in AI tasks; such as, visual question answeringand visual dialog.\nIdan Schwartz is a doctoral candidate in the Computer Science department at the Technion, working with Alexander G. Schwingand Tamir Hazan. In 2016 he joined eBay as a researcher, applying computer vision and natural language processing solutions for eBay\u0026rsquo;s catalog.","tags":null,"title":"Idan Schwartz","type":"authors"},{"authors":["I. Gat","I. Schwartz","A. ~G. Schwing"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633185707,"objectID":"faa27280a09e6ca47854593882dc3b6d","permalink":"https://idansc.github.io/publication/gat-neurips-2021/","publishdate":"2021-09-10T12:32:10.221016Z","relpermalink":"/publication/gat-neurips-2021/","section":"publication","summary":"Machine learning advances in the last decade have relied significantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classifiers, in turn, are prone to exploit those biases and to find shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we find a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classifiers to start quantifying perceptiveness via the proposed perceptual score.","tags":null,"title":"Perceptual Score: Measuring Perceptiveness of Multi-Modal Classifiers","type":"publication"},{"authors":["I. Schwartz"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633184885,"objectID":"cddc86c1cf3c792d9686cd902f6870fd","permalink":"https://idansc.github.io/publication/schwartz-naacl-2021/","publishdate":"2021-06-10T12:32:10.221016Z","relpermalink":"/publication/schwartz-naacl-2021/","section":"publication","summary":" Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., 'yeah' and 'yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as 'I don't know'. Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge.","tags":null,"title":"Ensemble of MRR and NDCG models for Visual Dialog","type":"publication"},{"authors":["I. Gat","I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633184885,"objectID":"20fd2862585e41ca548080f5d25df2e7","permalink":"https://idansc.github.io/publication/gat-neurips-2020/","publishdate":"2020-10-10T12:32:10.221016Z","relpermalink":"/publication/gat-neurips-2020/","section":"publication","summary":"Many recent datasets contain a variety of different data modalities, for instance, image, question, and answer data in visual question answering (VQA). When training deep net classifiers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classification results than others. This is suboptimal because the classifier is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classification result. However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2, and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efficacy of our method on Colored MNIST.","tags":null,"title":"Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies","type":"publication"},{"authors":["I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1560297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633184885,"objectID":"55b78618193469566ace21183ab34b41","permalink":"https://idansc.github.io/publication/schwartz-cvpr-2019/","publishdate":"2019-05-31T12:32:10.221016Z","relpermalink":"/publication/schwartz-cvpr-2019/","section":"publication","summary":"The recently proposed audio-visual scene-aware dialog task paves the way to a more data-driven way of learning virtual assistants, smart speakers and car navigation systems. However, very little is known to date about how to effectively extract meaningful information from a plethora of sensors that pound the computational engine of those devices. Therefore, in this paper, we provide and carefully analyze a simple baseline for audio-visual scene-aware dialog which is trained end-to-end. Our method differentiates in a data-driven manner useful signals from distracting ones using an attention mechanism. We evaluate the proposed approach on the recently introduced and challenging audio-visual scene-aware dataset, and demonstrate the key features that permit to outperform the current state-of-the-art by more than 20% on CIDEr.","tags":null,"title":"A Simple Baseline for Audio-Visual Scene-Aware Dialog","type":"publication"},{"authors":["I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1560297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633184885,"objectID":"6a3d011c4fef024bda78ce922f97099b","permalink":"https://idansc.github.io/publication/schwartz-fgacvpr-2019/","publishdate":"2019-05-31T12:35:14.854157Z","relpermalink":"/publication/schwartz-fgacvpr-2019/","section":"publication","summary":"Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.","tags":null,"title":"Factor Graph Attention","type":"publication"},{"authors":["I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1513209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633184885,"objectID":"b716db9682e082edae946aae30e3ef50","permalink":"https://idansc.github.io/publication/schwartz-nips-2017/","publishdate":"2019-05-31T12:35:14.854941Z","relpermalink":"/publication/schwartz-nips-2017/","section":"publication","summary":"The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.","tags":null,"title":"High-Order Attention Models for Visual Question Answering","type":"publication"}]