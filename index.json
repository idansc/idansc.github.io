[{"authors":["admin"],"categories":null,"content":"Idan\u0026rsquo;s research is centered around cognition in deep learning models. This includes attention models, multimodal problems, and multimodal bias removal. Specifically in AI tasks; such as, visual question answering and visual dialog.\nIdan Schwartz is a doctoral candidate in the Computer Science department at the Technion, working with Alexander G. Schwing and Tamir Hazan. In 2016 he joined eBay as a researcher, applying computer vision and natural language processing solutions for eBay\u0026rsquo;s catalog. In 2019 he joined Microsoft as a researcher at Search, Assistant, and Intelligence group. In 2020, he joined Spot (acquired by NetApp) to lead the research group.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602937000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://idansc.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Idan\u0026rsquo;s research is centered around cognition in deep learning models. This includes attention models, multimodal problems, and multimodal bias removal. Specifically in AI tasks; such as, visual question answering and visual dialog.\nIdan Schwartz is a doctoral candidate in the Computer Science department at the Technion, working with Alexander G. Schwing and Tamir Hazan. In 2016 he joined eBay as a researcher, applying computer vision and natural language processing solutions for eBay\u0026rsquo;s catalog.","tags":null,"title":"Idan Schwartz","type":"authors"},{"authors":["I. Gat","I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602937000,"objectID":"20fd2862585e41ca548080f5d25df2e7","permalink":"https://idansc.github.io/publication/gat-neurips-2020/","publishdate":"2020-10-10T12:32:10.221016Z","relpermalink":"/publication/gat-neurips-2020/","section":"publication","summary":"Many recent datasets contain a variety of different data modalities, for instance, image, question, and answer data in visual question answering (VQA). When training deep net classifiers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classification results than others. This is suboptimal because the classifier is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classification result. However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2, and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efficacy of our method on Colored MNIST.","tags":null,"title":"Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies","type":"publication"},{"authors":["I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1560297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602937000,"objectID":"55b78618193469566ace21183ab34b41","permalink":"https://idansc.github.io/publication/schwartz-cvpr-2019/","publishdate":"2019-05-31T12:32:10.221016Z","relpermalink":"/publication/schwartz-cvpr-2019/","section":"publication","summary":"The recently proposed audio-visual scene-aware dialog task paves the way to a more data-driven way of learning virtual assistants, smart speakers and car navigation systems. However, very little is known to date about how to effectively extract meaningful information from a plethora of sensors that pound the computational engine of those devices. Therefore, in this paper, we provide and carefully analyze a simple baseline for audio-visual scene-aware dialog which is trained end-to-end. Our method differentiates in a data-driven manner useful signals from distracting ones using an attention mechanism. We evaluate the proposed approach on the recently introduced and challenging audio-visual scene-aware dataset, and demonstrate the key features that permit to outperform the current state-of-the-art by more than 20% on CIDEr.","tags":null,"title":"A Simple Baseline for Audio-Visual Scene-Aware Dialog","type":"publication"},{"authors":["I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1560297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602937000,"objectID":"6a3d011c4fef024bda78ce922f97099b","permalink":"https://idansc.github.io/publication/schwartz-fgacvpr-2019/","publishdate":"2019-05-31T12:35:14.854157Z","relpermalink":"/publication/schwartz-fgacvpr-2019/","section":"publication","summary":"Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.","tags":null,"title":"Factor Graph Attention","type":"publication"},{"authors":["I. Schwartz","A. ~G. Schwing","T. Hazan"],"categories":null,"content":"","date":1513209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559310768,"objectID":"b716db9682e082edae946aae30e3ef50","permalink":"https://idansc.github.io/publication/schwartz-nips-2017/","publishdate":"2019-05-31T12:35:14.854941Z","relpermalink":"/publication/schwartz-nips-2017/","section":"publication","summary":"The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.","tags":null,"title":"High-Order Attention Models for Visual Question Answering","type":"publication"}]